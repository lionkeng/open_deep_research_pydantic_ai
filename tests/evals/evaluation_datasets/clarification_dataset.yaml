# Clarification Agent Evaluation Dataset
# This file contains test cases for evaluating the ClarificationAgent

dataset:
  name: clarification_agent_evaluation
  description: Comprehensive test cases for clarification agent evaluation
  version: "1.0.0"

# Test cases organized by category
cases:
  # Category: Multi-Question Test Cases (NEW)
  multi_question_cases:
    - name: broad_ml_research
      description: Broad ML query requiring multiple clarifications
      input:
        query: "I want to learn about machine learning"
      expected:
        needs_clarification: true
        min_questions: 2
        max_questions: 5
        expected_question_types: ["choice", "text"]
        dimension_categories:
          - audience_level
          - scope_focus
          - deliverable
        key_themes:
          - "specific area"
          - "technical level"
          - "practical application"
          - "learning goal"
      tags: ["multi-question", "broad", "learning"]

    - name: project_architecture
      description: Architecture query needing multiple aspects clarified
      input:
        query: "Help me design my system architecture"
      expected:
        needs_clarification: true
        min_questions: 3
        max_questions: 6
        expected_question_types: ["text", "choice", "multi_choice"]
        dimension_categories:
          - scope_focus
          - deliverable
        key_themes:
          - "system type"
          - "scale"
          - "requirements"
          - "technology stack"
      tags: ["multi-question", "architecture", "design"]

    - name: database_selection
      description: Database choice requiring multiple considerations
      input:
        query: "Which database should I use?"
      expected:
        needs_clarification: true
        min_questions: 2
        max_questions: 4
        expected_question_types: ["choice", "multi_choice", "text"]
        key_themes:
          - "data type"
          - "scale"
          - "performance"
          - "use case"
      tags: ["multi-question", "database", "selection"]

    - name: performance_optimization
      description: Performance query needing context on multiple dimensions
      input:
        query: "My application is slow, how can I optimize it?"
      expected:
        needs_clarification: true
        min_questions: 3
        max_questions: 5
        expected_question_types: ["text", "choice"]
        key_themes:
          - "application type"
          - "performance metrics"
          - "bottleneck"
          - "environment"
      tags: ["multi-question", "performance", "optimization"]

  # Category: Clear, Specific Queries (Should NOT need clarification)
  clear_queries:
    - name: bitcoin_price
      description: Specific financial query with clear intent
      input:
        query: "What is the current Bitcoin price in USD?"
      expected:
        needs_clarification: false
      tags: ["financial", "specific", "current"]

    - name: comparison
      description: Specific factual comparison with clear historical data
      input:
        query: "Compare the number of trophies that Brazil and Argentina have won in the Men's FIFA World Cup"
      expected:
        needs_clarification: false
      tags: ["factual", "comparison", "specific", "sports"]

    - name: code_implementation
      description: Clear programming task with specific requirements
      input:
        query: "How to implement binary search algorithm in Python with O(log n) complexity"
      expected:
        needs_clarification: false
      tags: ["programming", "algorithm", "specific"]

    - name: scientific_fact
      description: Specific scientific question
      input:
        query: "What is the speed of light in meters per second?"
      expected:
        needs_clarification: false
      tags: ["science", "fact", "specific"]

    - name: mathematical_calculation
      description: Clear mathematical calculation
      input:
        query: "Calculate 15% of 2500"
      expected:
        needs_clarification: false
      tags: ["math", "calculation", "specific"]

  # Category: Ambiguous Queries (SHOULD need clarification)
  ambiguous_queries:
    - name: broad_ai
      description: Very broad topic needing scope clarification
      input:
        query: "What is AI?"
      expected:
        needs_clarification: true
        dimension_categories:
          - audience_level
          - scope_focus
          - deliverable
        key_themes:
          - "artificial intelligence"
          - "specific aspect"
          - "technical level"
          - "purpose"
      tags: ["broad", "technology", "ambiguous"]

    - name: ambiguous_python
      description: Ambiguous term - could be programming language or animal
      input:
        query: "Tell me about Python"
      expected:
        needs_clarification: true
        dimension_categories:
          - scope_focus
        key_themes:
          - "programming language"
          - "snake"
          - "specific aspect"
      tags: ["ambiguous", "context-dependent"]

    - name: vague_research
      description: Broad research request needing focus
      input:
        query: "Research climate change"
      expected:
        needs_clarification: true
        dimension_categories:
          - scope_focus
          - deliverable
          - source_quality
        key_themes:
          - "specific aspect"
          - "geographic region"
          - "time period"
          - "purpose"
      tags: ["research", "broad", "vague"]

    - name: incomplete_context
      description: Missing subject reference
      input:
        query: "How does it work?"
      expected:
        needs_clarification: true
        dimension_categories:
          - scope_focus
        key_themes:
          - "what 'it' refers to"
          - "context"
          - "subject"
      tags: ["incomplete", "missing-context"]

    - name: broad_technology
      description: Extremely broad technology query
      input:
        query: "Explain technology"
      expected:
        needs_clarification: true
        dimension_categories:
          - audience_level
          - scope_focus
          - deliverable
        key_themes:
          - "specific technology"
          - "aspect"
          - "purpose"
      tags: ["broad", "technology"]

    - name: generic_help
      description: Generic help request
      input:
        query: "I need help"
      expected:
        needs_clarification: true
        dimension_categories:
          - scope_focus
        key_themes:
          - "specific problem"
          - "area"
          - "assistance"
      tags: ["vague", "help"]

  # Category: Edge Cases
  edge_cases:
    - name: minimal_query
      description: Minimal query with no content
      input:
        query: "?"
      expected:
        needs_clarification: true
        key_themes:
          - "question"
          - "help"
          - "clarify"
      tags: ["edge-case", "minimal"]

    - name: multiple_questions
      description: Multiple questions needing prioritization
      input:
        query: "What is machine learning and how does it compare to deep learning and can you also explain neural networks?"
      expected:
        needs_clarification: true
        dimension_categories:
          - scope_focus
          - deliverable
        key_themes:
          - "which topic"
          - "prioritize"
          - "depth"
          - "focus"
      tags: ["multiple", "complex"]

    - name: typo_query
      description: Query with typos but clear intent
      input:
        query: "Wat is teh curent temprature in New York?"
      expected:
        needs_clarification: false
      tags: ["typo", "clear-intent"]

    - name: non_english_simple
      description: Simple non-English query
      input:
        query: "Bonjour"
      expected:
        needs_clarification: true
        key_themes:
          - "language"
          - "help"
          - "assistance"
      tags: ["non-english", "greeting"]

  # Category: Context-Dependent Queries
  context_dependent:
    - name: with_prior_context
      description: Query that makes sense with context
      input:
        query: "What about the performance?"
        context:
          - role: user
            content: "Tell me about Tesla Model 3"
          - role: assistant
            content: "The Tesla Model 3 is an electric sedan..."
      expected:
        needs_clarification: false
      tags: ["context", "follow-up"]

    - name: ambiguous_followup
      description: Ambiguous follow-up question
      input:
        query: "What about the other one?"
        context:
          - role: user
            content: "Compare iPhone and Android"
      expected:
        needs_clarification: true
        key_themes:
          - "which"
          - "specific"
          - "clarify"
      tags: ["context", "ambiguous"]

# Evaluation Configuration
evaluation:
  evaluators:
    - name: BinaryAccuracyEvaluator
      description: Evaluates binary correctness of clarification decision
      weight: 1.0

    - name: DimensionCoverageEvaluator
      description: Evaluates coverage of 4-dimension framework
      weight: 0.8
      applies_to: ["ambiguous_queries", "edge_cases"]

    - name: QuestionRelevanceEvaluator
      description: Evaluates relevance and quality of questions
      weight: 0.9
      applies_to: ["ambiguous_queries", "multi_question_cases"]

    - name: MultiQuestionEvaluator
      description: Evaluates multi-question generation capabilities
      weight: 1.0
      applies_to: ["multi_question_cases"]

    - name: ConsistencyEvaluator
      description: Evaluates consistency across multiple runs
      weight: 0.7
      config:
        num_runs: 3

    - name: LLMJudgeEvaluator
      description: Uses LLM to judge clarification quality
      weight: 0.6
      config:
        model: "gpt-4o-mini"
      applies_to: ["ambiguous_queries"]

# Metrics to track
metrics:
  primary:
    - accuracy: Binary classification accuracy
    - f1_score: F1 score for clarification detection
    - precision: Precision of clarification detection
    - recall: Recall of clarification detection

  secondary:
    - dimension_coverage: Average coverage of 4-dimension framework
    - question_quality: Average quality score of clarification questions
    - consistency_score: Consistency across multiple runs
    - response_time: Average response time in milliseconds
    - avg_question_count: Average number of questions generated
    - question_type_diversity: Diversity of question types used
    - required_optional_ratio: Ratio of required to optional questions

  qualitative:
    - common_failure_patterns: List of common failure cases
    - dimension_distribution: Distribution of identified dimensions
    - question_patterns: Common patterns in clarification questions
